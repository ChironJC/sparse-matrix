#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul  1 09:12:41 2019

@author: jiachenlu
"""


import Sparse_2 as sp
import tensorflow as tf
import numpy as np
import os
import matplotlib.pyplot as plt


learning_rate = 0.95
'''
adj is used to record the non-zeros in the upper part of the matrix 
    for each colomn, length is half of the total non-zeros
xadj is used to record the number of non-xeros in the upper part of the matrix 
    for each colomn, length is the number of colomns
order is used to record the current permutation of vertices, it is a permutation
    of range(len(xadj))
separator is used to record the index of vertices in separator for the initial matrix
'''
n = 25
sparsity = 0.5
A = [[1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],
     [0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],
     [0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],
     [0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0],
     [0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0],
     [0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0],
     [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0],
     [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1],
     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1]]
adj, xadj = sp.upperSpar(A)
sp.printSparse(adj, xadj)
#adj , xadj = sp.getConSpar(adj, xadj)
separator = {1,5}
length = len(adj)
    
f = open("data.txt", "w")
f.write(str(adj))
f.write(str(xadj)+'\n')


def preprocess(separator, adj, xadj):
    zeros = [0 for i in range(length)]
    states = np.array([zeros,zeros,zeros])
    state = []
    for i in range(n):
        if i in separator:
            state.append(1)
        else:
            state.append(0)
    states[0][:len(state)] = state
    states[1] = adj
    states[2][:len(xadj)] = xadj
    return states.reshape(1, n_input, length)

n_input = 3 #separator, adj, xadj
n_action = n
n_hidden = int(2*sparsity*n**2)
n_output = int(n_action)
hidden_activation = tf.nn.relu
initializer = tf.contrib.layers.variance_scaling_initializer()

def q_network(x_state, scope):
    prev_layers = [0]*n_input
    for i in range(n_input):
        prev_layers[i] = x_state[:,i,:]
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:
        outputs = [0]*n_input
        for i in range(n_input):
            hidden = tf.contrib.layers.fully_connected(
                prev_layers[i], n_hidden, activation_fn=hidden_activation,
                weights_initializer=initializer)
            outputs[i] = tf.contrib.layers.fully_connected(
                hidden, n_output, activation_fn=hidden_activation,
                weights_initializer=initializer)
        middle_layer = np.sum(outputs)
        hidden_1 = tf.contrib.layers.fully_connected(
                middle_layer, n_output, activation_fn=hidden_activation,
                weights_initializer=initializer)
        hidden_2 = tf.contrib.layers.fully_connected(
                hidden_1, n_output, activation_fn=hidden_activation,
                weights_initializer=initializer)
        hidden_3 = tf.contrib.layers.fully_connected(
                hidden_2, n_output, activation_fn=hidden_activation,
                weights_initializer=initializer)
        output = tf.contrib.layers.fully_connected(
                hidden_3, n_output, activation_fn=None,
                weights_initializer=initializer)
        separator = x_state[:,0,:n_output]
        print(np.shape(separator))
        new_output= np.multiply(output, separator)
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                       scope=scope.name)
    trainable_vars_by_name = {var.name[len(scope.name):]: var
                                       for var in trainable_vars}
    return new_output, trainable_vars_by_name

x_state = tf.placeholder(tf.float32, shape=[None,n_input, length])
actor_q_values, actor_vars = q_network(x_state, scope="q_network/actor")
critic_q_values, critic_vars = q_network(x_state, scope="q_network/critic")

copy_ops = [actor_var.assign(critic_vars[var_name])
            for var_name, actor_var in actor_vars.items()]
copy_critic_to_actor = tf.group(*copy_ops)

x_action = tf.placeholder(tf.int32, shape=[None])
q_value = tf.reduce_sum(critic_q_values * tf.one_hot(x_action, n_output),
                        axis=1, keep_dims=True)
#q_values = tf.squeeze(q_value)

y = tf.placeholder(tf.float32, shape=[None,1])
cost = tf.reduce_mean(tf.square(y-q_value))
global_step = tf.Variable(0, trainable=False, name="global_step")
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(cost, global_step=global_step)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
saver1 = tf.train.Saver(var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = "q_network/actor"))


from collections import deque

replay_memory_size = 1000
replay_memory = deque([], maxlen=replay_memory_size)

def sample_memories(batch_size):
    indices = np.random.permutation(min(len(replay_memory),10*batch_size))[:batch_size]
    cols = [[], [], [], []] #state, action, reward, next_state
    for idx in indices:
        memory = replay_memory[len(replay_memory)-idx-1]
        for col, value in zip(cols,memory):
            col.append(value)
    cols = [np.array(col) for col in cols]
    return (cols[0], cols[1], cols[2].reshape(-1, 1),
            cols[3])

eps_min = 0.1
eps_max = 1.0
eps_decay_steps = 1000

def epsilon_greedy(q_values, step, separator):
    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)
    if np.random.rand() < epsilon:
        f.write('random')
        return np.random.randint(0, n_action)
    else:
        return np.argmax(q_values)
    
n_steps = 1500
training_start = 100
training_interval = 5
save_steps = 50
copy_steps = 25
discount_rate = 0.70
batch_size = 50
iteration = 0
checkpoint_path = "./my_dqn.ckpt"
test_path = "./my_test.ckpt"
safe_adj = adj.copy()
print(len(safe_adj))
safe_xadj = xadj.copy()
iteration_v = np.arange(1500)
q_v = [0]*1500

with tf.Session() as sess:
    if os.path.isfile(checkpoint_path):
        saver.restore(sess, checkpoint_path)
    else:
        init.run()
    while True:
        step= global_step.eval()
        if step >= n_steps:
            break
        iteration += 1
        
        state = preprocess(separator, adj, xadj)
        q_values = actor_q_values.eval(feed_dict={x_state: state})
        action = epsilon_greedy(q_values, step, separator)
        
        adj = safe_adj.copy()
        xadj = safe_xadj.copy()
        next_separator, reward = sp.step(
                action, adj, xadj, separator)
        adj = safe_adj.copy()
        xadj = safe_xadj.copy()
        next_state = preprocess(next_separator, adj, xadj)
        replay_memory.append((state,
                              action,
                              reward,
                              next_state))
        
        if reward != 0:
            for i in range(1,8):
                n_turn = i%4
                n_reflect = (i-n_turn)/4
                
                add_separator = {sp.add(n_turn, n_reflect, ver) 
                    for ver in separator}
                add_next_separator = {sp.add(n_turn, n_reflect, ver) 
                    for ver in next_separator}
                add_action = sp.add(n_turn, n_reflect, action)
                                
                add_state = preprocess(add_separator, adj, xadj)
                add_action = add_action
                add_reward = reward
                add_next_state = preprocess(add_next_separator, adj, xadj)

        separator = next_separator
        
        f.write(str(action))
        f.write(' ')
        f.write(str(separator))
        f.write(' ')
        f.write(str(reward))
        f.write('\n')
        
        if iteration < training_start or iteration % training_interval != 0:
            continue
        
        x_state_val, x_action_val, rewards, x_next_state_val = sample_memories(
                batch_size)
        x_state_val = tf.squeeze(x_state_val)
        x_state_val = x_state_val.eval()
        x_next_state_val = tf.squeeze(x_next_state_val)
        x_next_state_val = x_next_state_val.eval()
        next_q_values = actor_q_values.eval(
                feed_dict={x_state: x_next_state_val})
        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)
        y_val = rewards + discount_rate * max_next_q_values
        for i in range(len(rewards)):
            if rewards[i] == 0:
                y_val[i] = max_next_q_values[i]
            else:
                y_val[i] = rewards[i] + discount_rate * max_next_q_values[i]
            
        critic_q_values.eval(feed_dict={x_state: x_state_val})

        training_op.run(feed_dict={x_state: x_state_val,
                                   x_action: x_action_val,
                                   y: y_val})
        print(step)
        q_v[step] = max(y_val)
        plt.plot(iteration_v, q_v)
        plt.yscale("log")
        plt.show()
        if step % copy_steps == 0:
            copy_critic_to_actor.run()
        
        if step % save_steps == 0:
            saver.save(sess, checkpoint_path)
            saver1.save(sess, test_path)
f.close()